{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time speech recognition\n",
    "### Or: Why Voice Assistants don't work properly..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By: Aaron Alef (Email: aaron.alef@code.berlin - Slack: [@aaron](https://slack.com/app_redirect?team=T54B2S3T9&channel=U82F166U9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**It can be hard to listen for a long time and it's easy to miss details... So why not make use of our omnipresent smartphones to support us?  \n",
    "We want to find out where the limits of our microphones are when it comes to the core of our project - audio transcription!  \n",
    "Using machine learning, specifically neuronal networks, we search for words in audio streams.  \n",
    "Result: Real-time transcription is *hard!***\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [this Medium article](https://medium.com/descript/which-automatic-transcription-service-is-the-most-accurate-2018-2e859b23ed19) Google currently achieves the highest transcription accuracy, with a Word-Error-Rate (short WER) of 16% - low enough to make sense of the transcript and high enough to annoy potential customers. Google in particular is focussing on the recognition of speech commands - that is, maximising the potential of their Google Voice Assistant, aiming on making it accurate and fast, as mentioned [here](https://www.wired.com/story/google-made-truly-usable-voice-assistant/), and so do other companies like Amazon or Microsoft.  \n",
    "However, none of them really focus on actual *speech* recognition.\n",
    "On the `transcription-gcloud` branch you can try out it's accuracy - it works, but isn't really satisfying to use.  \n",
    "So, we asked ourselves, what makes it so hard?  \n",
    "And how far can we come with an own algorithm adapted to be used on speeches?  \n",
    "To answer these questions we started working on our own speech-to-text converter, using the open-source [Common Voice](https://voice.mozilla.org/en) data set crowdsourced by Mozilla.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to provide a tool that\n",
    "\n",
    "1. Takes a continous stream of audio data with unknown length\n",
    "2. converts this speech into meaningful sentences\n",
    "3. Gives these sentences back, as sequential stream of single words\n",
    "\n",
    "A server would handle the connection to the mobile phone, using gRPC, a real-time streaming protocol.\n",
    "But as this is about the language processing part, I will leave out unimportant details about where the audio comes from and focus on its processing only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "The continous audio stream means our tool gets small chunks of audio data, a few hundred samples, as an array, at a time.\n",
    "A first dive into the matter resulted in the following plan:\n",
    "\n",
    "1. Extract features from one chunk of data, called window.\n",
    "2. Feed these features into a neural network trained on the aforementioned data set; the result being single phonemes.\n",
    "3. Put these phonemes back together to meaningful words\n",
    "\n",
    "The feature extraction part is easy.\n",
    "Using the python library `librosa` we calculated the mfcc on each of these windows, that is, the mel-frequency cepstrum coefficients. These coefficients would then be our features.  \n",
    "Internally, this would do a fourier transform (Split the sound into its frequencies) mapped onto a mel-scale plus some further processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcription import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
